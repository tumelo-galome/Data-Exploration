{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca2a6d32",
   "metadata": {},
   "source": [
    " <h1> Joined Assignment Python and Data Analytics </h1>\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343d917",
   "metadata": {},
   "source": [
    "<h2> Final Code </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe83d3e",
   "metadata": {},
   "source": [
    "Used Sources\n",
    "\n",
    "- https://stackoverflow.com/questions/19960077/how-to-filter-pandas-dataframe-using-in-and-not-in-like-in-sql\n",
    "- https://kanoki.org/2020/01/21/pandas-dataframe-filter-with-multiple-conditions/\n",
    "- https://stackoverflow.com/questions/21271581/selecting-pandas-columns-by-dtype\n",
    "- https://www.datasciencemadesimple.com/strip-space-column-pandas-dataframe-leading-trailing-2/\n",
    "- https://stackoverflow.com/questions/23208745/adding-dummy-columns-to-the-original-dataframe\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364fb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "'-------------------------------------------Step 1 - Loading packages------------------------------------------------'\n",
    "\n",
    "# import needed packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'-------------------------------------------Step 2 - Loading data----------------------------------------------------'\n",
    "\n",
    "# filename --> change here for different name\n",
    "file = './__datasets/Team 3 - US Census Above 50k Predictor.csv'\n",
    "\n",
    "# creating header names\n",
    "headers = ['age', 'job', 'type_of_employment', 'working_hours_per_week', 'return_of_investment',\n",
    "'level_education', 'years_education', 'marital_status', 'relationship_household', 'race',\n",
    "'gender', 'nationality', 'yearly_income_bigger_50k']\n",
    "\n",
    "# looking for na_values\n",
    "na_values = [' ?', '?']\n",
    "\n",
    "# using pandas to load data\n",
    "df = pd.read_csv(filepath_or_buffer = file,\n",
    "                 header             = None,\n",
    "                 names              = headers,\n",
    "                 na_values          = na_values,\n",
    "                 keep_default_na    = True)\n",
    "\n",
    "# getting rid of leading spaces in object columns\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "'-------------------------------------------Step 3 - Translate values------------------------------------------------'\n",
    "\n",
    "# new dataframe for translating the columns\n",
    "df_translated = pd.DataFrame.copy(df)\n",
    "\n",
    "# translating the categorical columns into classes\n",
    "# level of education\n",
    "all_degrees = {'High_School_Graduate':['HS-grad'], \n",
    "               'Bachelors':['Some-college', 'Bachelors', 'Prof-school'],\n",
    "               'Compulsory':['11th', '10th', '7th-8th', '9th', '12th', '5th-6th', '1st-4th', 'Preschool'],\n",
    "               'Associate':['Assoc-voc', 'Assoc-acdm'],\n",
    "               'University':['Masters', 'Doctorate']}\n",
    "\n",
    "# translating\n",
    "for degree in all_degrees:\n",
    "    df_translated.loc[df_translated.level_education.isin(values=all_degrees[degree]), 'level_education'] = degree\n",
    "\n",
    "# marital status\n",
    "all_status = {'Single': ['Never-married', 'Widowed', 'Divorced'], \n",
    "             'Married': ['Married-spouse-absent', 'Married-civ-spouse', 'Married-AF-spouse', 'Separated']}\n",
    "        \n",
    "# translating\n",
    "for status in all_status:\n",
    "    df_translated.loc[df_translated.marital_status.isin(values=all_status[status]), 'marital_status'] = status\n",
    "\n",
    "# relationship households\n",
    "all_households = {'House_holder':['Wife', 'Husband', 'Unmarried', 'Other-relative', 'Own-child'],\n",
    "                  'Household_Non_Family':['Not-in-family']}\n",
    "\n",
    "# translating\n",
    "for household in all_households:\n",
    "    df_translated.loc[df_translated.relationship_household.isin(values=all_households[household]), 'relationship_household'] = household\n",
    "\n",
    "# type of employment\n",
    "all_employments = {'Private': ['Private'],\n",
    "                   'Goverment-Employeed': ['Local-gov', 'State-gov', 'Federal-gov'],\n",
    "                   'Other' : ['Self-emp-not-inc', 'Self-emp-inc', 'Without-pay', 'Never-worked']}\n",
    "\n",
    "# translating\n",
    "for employ in all_employments:\n",
    "    df_translated.loc[df_translated.type_of_employment.isin(values=all_employments[employ]), 'type_of_employment'] = employ\n",
    "\n",
    "    \n",
    "# nationlity --> all except USA and NaN to Other\n",
    "df_translated.loc[(df_translated['nationality'] != 'United-States') & (df_translated['nationality'].notnull()),'nationality'] = 'Other'\n",
    "\n",
    "# race\n",
    "all_race = {'White': ['White'],\n",
    "            'Person_of_Color': ['Black', 'Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Other']}\n",
    "\n",
    "# translating\n",
    "for race in all_race:\n",
    "    df_translated.loc[df_translated.race.isin(values=all_race[race]), 'race'] = race\n",
    "\n",
    "df_translated['race'].value_counts()\n",
    "\n",
    "# job\n",
    "all_job = {'Other': ['Other-service'],\n",
    "            'Manual labor': ['Machine-op-inspct', 'Handlers-cleaners', 'Priv-house-serv', 'Farming-fishing', 'Transport-moving', 'Craft-repair'],\n",
    "          'Opertional_and_Tech': ['Exec-managerial', 'Sales', 'Prof-specialty', 'Adm-clerical', 'Tech-support'],\n",
    "          'Security': ['Armed-Forces', 'Protective-serv']}\n",
    "\n",
    "# translating\n",
    "for job in all_job:\n",
    "    df_translated.loc[df_translated.job.isin(values=all_job[job]), 'job'] = job\n",
    "\n",
    "df_translated['job'].value_counts()\n",
    "\n",
    "'-------------------------------------------Step 4 - Imputing values-------------------------------------------------'\n",
    "\n",
    "# declaring all datatypes for later\n",
    "data_types = {'age': int, 'job':str, 'type_of_employment':str, 'working_hours_per_week':int, \n",
    "              'return_of_investment':int, 'level_education':str, 'years_education':int, 'marital_status':str,\n",
    "              'relationship_household':str, 'race':str, 'gender':str, 'nationality':str, \n",
    "              'yearly_income_bigger_50k':str}\n",
    "\n",
    "#getting all numerical columns\n",
    "num_cols = []\n",
    "for col in data_types:\n",
    "    if data_types[col] == int:\n",
    "        num_cols.append(col)\n",
    "        \n",
    "#getting all categorical columns       \n",
    "cat_cols = []\n",
    "for col in data_types:\n",
    "    if data_types[col] == str:\n",
    "        cat_cols.append(col)\n",
    "\n",
    "# numerical columns --> impute with the mean\n",
    "for col in df_translated[num_cols].columns:\n",
    "    numerical_mean = int(df_translated[col].mean())\n",
    "    df_translated[col].fillna(value = numerical_mean,\n",
    "                             inplace = True)\n",
    "    \n",
    "#categorical columns\n",
    "# these columns we want to change by hand\n",
    "handmade_columns = ['job', 'nationality']\n",
    "\n",
    "# changing not handmade columns --> mode\n",
    "for col in df_translated[cat_cols].drop(labels=handmade_columns, axis=1).columns:\n",
    "    # getting the mode\n",
    "    categorical_mode = df_translated[col].mode()[0]\n",
    "    # filling NAs with Mode\n",
    "    df_translated[col].fillna(value = categorical_mode,\n",
    "                             inplace = True) \n",
    "\n",
    "# handmade changes\n",
    "# imputing nationlity using race\n",
    "dict_nat_race = {}\n",
    "\n",
    "# for all races get most appearing nationality\n",
    "for race in df_translated['race'].unique():\n",
    "    input_value = df_translated[['race', 'nationality']].value_counts()[race].idxmax()\n",
    "    dict_nat_race[race] = input_value\n",
    "       \n",
    "for race in dict_nat_race:\n",
    "    df_translated.loc[(df_translated['race'] == race) & (df_translated['nationality'].isnull()),'nationality'] = dict_nat_race[race]\n",
    "\n",
    "# imputing job using education\n",
    "dict_job_ed = {}\n",
    "\n",
    "for ed in df_translated['level_education'].unique():\n",
    "    input_value = df_translated[['level_education', 'job']].value_counts()[ed].idxmax()\n",
    "    dict_job_ed[ed] = input_value\n",
    "        \n",
    "for ed in dict_job_ed:\n",
    "    df_translated.loc[(df_translated['level_education'] == ed) & (df_translated['job'].isnull()),'job'] = dict_job_ed[ed]\n",
    "    \n",
    "'-------------------------------------------Step 5 - Changing Datatypes----------------------------------------------'\n",
    "\n",
    "# change all datatypes with dictionary\n",
    "df_translated.astype(dtype = data_types)\n",
    "\n",
    "'-------------------------------------------Step 6 - Generating Dummy Variables--------------------------------------'\n",
    "\n",
    "#generating dummy variables for each categorical column, dropping the first column, adding prefix with column name\n",
    "for col in df_translated.select_dtypes(include=['object']).columns:\n",
    "    df_translated = pd.concat(objs=[df_translated, \n",
    "                                    pd.get_dummies(data=df_translated[col], \n",
    "                                                   drop_first=True, \n",
    "                                                   prefix = f\"\"\"d_{col}\"\"\")\n",
    "                                   ], \n",
    "                              axis=1)\n",
    "\n",
    "'-------------------------------------------Step 7 - Generating Bins-------------------------------------------------'\n",
    "#creating bins for numerical columns using quartiles/quantiles\n",
    "for col in df_translated.select_dtypes(include=['int']).columns:\n",
    "    #cutting data into quartiles\n",
    "    try:\n",
    "        df_translated[f'b_{col}'] = pd.qcut(x=df_translated[col], q=4, labels = [0, 1, 2, 3])\n",
    "    #cutting into quartiles of the same size is not possible\n",
    "    except:\n",
    "        # getting the quantile values for the column\n",
    "        quantiles = np.quantile(df_translated[col].unique(), q=[0, 0.25, 0.5, 0.75, 1])\n",
    "        quantiles[0] -= 1\n",
    "\n",
    "        # producing the bins\n",
    "        df_translated[f'b_{col}'] = pd.cut(x = df_translated[col], bins=quantiles, labels=[0,1,2,3])    \n",
    "\n",
    "'-------------------------------------------Step 8 - Creating train and test sets------------------------------------'  \n",
    "\n",
    "# isolating target value into new df\n",
    "# getting columns for y\n",
    "if 'd_yearly_income_bigger_50k_>50K' in df_translated.columns:\n",
    "    y_cols = ['d_yearly_income_bigger_50k_>50K', 'yearly_income_bigger_50k']\n",
    "elif 'd_yearly_income_bigger_50k_<=50K':\n",
    "    y_cols = ['d_yearly_income_bigger_50k_<=50K', 'yearly_income_bigger_50k']\n",
    "else:\n",
    "    print('Missing column for prediction')\n",
    "    \n",
    "y = df_translated.loc[:,y_cols]\n",
    "# dropping target value from original df\n",
    "df = df_translated.drop(labels=y_cols, axis=1)\n",
    "\n",
    "\n",
    "#creating test and train sets using sklearn\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_translated, y, test_size=test_size, stratify=y, random_state=1)\n",
    "\n",
    "\n",
    "X_train.to_excel(excel_writer='train_x.xlsx', sheet_name='train_x')\n",
    "X_test.to_excel(excel_writer='test_x.xlsx', sheet_name='test_x')\n",
    "y_train.to_excel(excel_writer='train_y.xlsx', sheet_name='train_y')\n",
    "y_test.to_excel(excel_writer='test_y.xlsx', sheet_name='test_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7658d2f3",
   "metadata": {},
   "source": [
    " <h1> Joined Assignment Python and Data Analytics </h1>\n",
    "    \n",
    " <h2> Loading DataFrame into python </h2>\n",
    "     \n",
    " <h3> Importing packages </h3>\n",
    "\n",
    "Our start point for preparing the DataFrame to be analized is import the packages pandas (pd), pyplot (plt), numpy (np), seaborn (sns) and train_test_split from sklearn since we will use use pandas to read in and convert the data, pyplot and seaborn to visualize the data, numpy for creating quantiles, and sklearn to split our dataframe into a test and training set. This task will be conducted using Code 2.1.1 in our final code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ced24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 2.1.1\n",
    "\n",
    "# importing needed packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135c2a6",
   "metadata": {},
   "source": [
    " <h3> Load Data with pandas </h3>\n",
    "First, we specified a file name. That variable has to be changed whenever the name or saving location changes. After loading it once, we realized there were no headers in the csv. We then specified the column titles in the list <em>headers</em>, see Code 2.1.2.1.\n",
    "In the next step we checked if we have to convert categorical values into <em>NaN</em>'s. For doing so, we used the following script:\n",
    "\n",
    "~~~\n",
    "for col in df.columns:\n",
    "    print(df[col].value_counts())\n",
    "~~~\n",
    "\n",
    "We found out that missing values are represented with a <em> ? </em>. After realizing that the question mark comes with a space, we declared a list <em>na_values</em> for both cases. We then read in the csv using pandas <em>read_csv</em>-function and passing in our <em>headers</em> headers and <em>na_values</em> lists. To check if our convertion went right, we ran the following script knowing that the column <em> job </em> had question marks:\n",
    "\n",
    "~~~\n",
    "print(df['job'][df.loc[:, 'job'].isnull() == True ])\n",
    "~~~\n",
    "\n",
    "That resulted in a dataframe consisting of 1449 rows showing all missing values of the column. In the next section, we will investigate the quality of the data and deal with these values.\n",
    "Last but not least we discoverd that there are leading spaces in every categorical column. We got rid of these with the following code:\n",
    "\n",
    "~~~\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = df[col].str.strip()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65eae03",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Code 2.1.2.1\n",
    "\n",
    "# Looading data\n",
    "# filename --> change here for different name\n",
    "file = './__datasets/Team 3 - US Census Above 50k Predictor.csv'\n",
    "\n",
    "# creating header names\n",
    "headers = ['age', 'job', 'type_of_employment', 'working_hours_per_week', 'return_of_investment',\n",
    "'level_education', 'years_education', 'marital_status', 'relationship_household', 'race',\n",
    "'gender', 'nationality', 'yearly_income_bigger_50k']\n",
    "\n",
    "# looking for na_values\n",
    "na_values = [' ?', '?']\n",
    "\n",
    "# using pandas to load data\n",
    "df = pd.read_csv(filepath_or_buffer = file,\n",
    "                 header             = None,\n",
    "                 names              = headers,\n",
    "                 na_values          = na_values,\n",
    "                 keep_default_na    = True)\n",
    "\n",
    "# getting rid of leading spaces in object columns\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = df[col].str.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dcfaad",
   "metadata": {},
   "source": [
    " <h2> Investigating the columns </h2> \n",
    " \n",
    " <h3> Frequency of values </h3> \n",
    " \n",
    " To get a better understanding of the data we checked the columns. First we analyzed, how many unique values every categorical column has. We found out that there are 14 different jobs, 16 different levels of education and more than 40 nationalities. Creating dummy variables for all those categories will affect the outcome of the model as it will lose degrees of freedom. To evaluate that more, we looked into and plotted the frequency of each categorical column allowing us to make assumptions and create new columns.\n",
    "Our first approach was cutting all the values under 10% off and put them into the <em> other </em> category. We plotted that on the middle chart, but by doing so we realized that the new category does not provide a rational categorization. \n",
    "\n",
    "<h4> Job </h4>\n",
    "We decided to create four categories according to the type of work: (1) manual labor, (2) operational and tech, (3) security, and (4) other. \n",
    "\n",
    "<h4> Type of Employment </h4>\n",
    "We grouped the types of employment into three categories: (1) government, (2) private, and (3) unemployed and other. The <em>other</em> category includes self employed persons and unpaid employees.\n",
    "\n",
    "<h4> Level of Education </h4>\n",
    "We grouped the level of education into five mayor buckets: (1) university, (2) bachelors, (3) associate, (4) high-school-graduate, and (5) compulsory. This categorization was clear post reading reading the <em>Subject Definitions</em> document.\n",
    "\n",
    "<h4> Marital Status </h4>\n",
    "We created two categories: (1) single, and (2) married. If the individual had never been married, widowed or divorced, we put them into the single category, else into married.\n",
    "\n",
    "<h4> Relationship Household </h4>\n",
    "All persons except the not in family people were grouped into the category householders.\n",
    "\n",
    "<h4> Race </h4>\n",
    "We decided to create two categories: (1) white, and (2) other for those who does not identify as white. \n",
    "\n",
    "<h4> Nationality </h4>\n",
    "We decided to create two categories: (1) United States, and (2) other. This was due to the fact that individuals with nationality different from United States were not represented enough to create subcategories for all the unique values found. \n",
    "\n",
    "You can see the results of that relabeling in the third chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d06e694",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code 2.2.1.1\n",
    "\n",
    "# new dataframe for translating the columns\n",
    "df_translated = pd.DataFrame.copy(df)\n",
    "\n",
    "# translating the categorical columns into classes\n",
    "# level of education\n",
    "all_degrees = {'High_School_Graduate':['HS-grad'], \n",
    "               'Bachelors':['Some-college', 'Bachelors', 'Prof-school'],\n",
    "               'Compulsory':['11th', '10th', '7th-8th', '9th', '12th', '5th-6th', '1st-4th', 'Preschool'],\n",
    "               'Associate':['Assoc-voc', 'Assoc-acdm'],\n",
    "               'University':['Masters', 'Doctorate']}\n",
    "\n",
    "# translating\n",
    "for degree in all_degrees:\n",
    "    df_translated.loc[df_translated.level_education.isin(values=all_degrees[degree]), 'level_education'] = degree\n",
    "\n",
    "# marital status\n",
    "all_status = {'Single': ['Never-married', 'Widowed', 'Divorced'], \n",
    "             'Married': ['Married-spouse-absent', 'Married-civ-spouse', 'Married-AF-spouse', 'Separated']}\n",
    "        \n",
    "# translating\n",
    "for status in all_status:\n",
    "    df_translated.loc[df_translated.marital_status.isin(values=all_status[status]), 'marital_status'] = status\n",
    "\n",
    "# relationship households\n",
    "all_households = {'House_holder':['Wife', 'Husband', 'Unmarried', 'Other-relative', 'Own-child'],\n",
    "                  'Household_Non_Family':['Not-in-family']}\n",
    "\n",
    "# translating\n",
    "for household in all_households:\n",
    "    df_translated.loc[df_translated.relationship_household.isin(values=all_households[household]), 'relationship_household'] = household\n",
    "\n",
    "# type of employment\n",
    "all_employments = {'Private': ['Private'],\n",
    "                   'Goverment-Employeed': ['Local-gov', 'State-gov', 'Federal-gov'],\n",
    "                   'Other' : ['Self-emp-not-inc', 'Self-emp-inc', 'Without-pay', 'Never-worked']}\n",
    "\n",
    "# translating\n",
    "for employ in all_employments:\n",
    "    df_translated.loc[df_translated.type_of_employment.isin(values=all_employments[employ]), 'type_of_employment'] = employ\n",
    "    \n",
    "# nationality --> all except USA and NaN to Other\n",
    "df_translated.loc[(df_translated['nationality'] != 'United-States') & (df_translated['nationality'].notnull()),'nationality'] = 'Other'\n",
    "\n",
    "# race\n",
    "all_race = {'White': ['White'],\n",
    "            'Person_of_Color': ['Black', 'Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Other']}\n",
    "\n",
    "# translating\n",
    "for race in all_race:\n",
    "    df_translated.loc[df_translated.race.isin(values=all_race[race]), 'race'] = race\n",
    "\n",
    "df_translated['race'].value_counts()\n",
    "\n",
    "# job\n",
    "all_job = {'Other': ['Other-service'],\n",
    "            'Manual labor': ['Machine-op-inspct', 'Handlers-cleaners', 'Priv-house-serv', 'Farming-fishing', 'Transport-moving', 'Craft-repair'],\n",
    "          'Opertional_and_Tech': ['Exec-managerial', 'Sales', 'Prof-specialty', 'Adm-clerical', 'Tech-support'],\n",
    "          'Security': ['Armed-Forces', 'Protective-serv']}\n",
    "\n",
    "# translating\n",
    "for job in all_job:\n",
    "    df_translated.loc[df_translated.job.isin(values=all_job[job]), 'job'] = job\n",
    "\n",
    "df_translated['job'].value_counts()\n",
    "\n",
    "# create dataframe with all categorical columns for plotting\n",
    "df_objects = pd.DataFrame.copy(df).select_dtypes(include=['object'])\n",
    "df_objects_other = pd.DataFrame.copy(df).select_dtypes(include=['object'])\n",
    "\n",
    "\n",
    "\n",
    "# check how many unique values each column has, plot the different dataframes to compare value count\n",
    "for col in df_objects.columns:\n",
    "    \n",
    "    # counting\n",
    "    counts = df_objects[col].value_counts()\n",
    "    print(f\"\"\"Column {col}: {counts.count()} different values\n",
    "Biggest category ({counts.idxmax()}) has {counts.max()} values\n",
    "Smallest category ({counts.idxmin()}) has {counts.min()} values\"\"\")\n",
    "    \n",
    "    # imputing other for all values with less than 10% in column frequency\n",
    "    counts = df_objects_other[col].value_counts()/df_objects[col].count()\n",
    "    mask = df_objects_other[col].isin(counts[counts<0.1].index)\n",
    "    df_objects_other.loc[mask,col] = 'other'\n",
    "    \n",
    "    # 2 graphs side by side\n",
    "    fig, ax =plt.subplots(1,3,figsize=(12, 3)) \n",
    "    \n",
    "    # graph 1\n",
    "    sns.countplot(data = df_objects, x = col, color = 'blue',\n",
    "             order = df_objects[col].value_counts().index, ax=ax[0])\n",
    "    ax[0].set_title(f\"\"\"{col} original\"\"\")\n",
    " \n",
    "    # graph 2\n",
    "    sns.countplot(data = df_objects_other, x = col, color = 'red',\n",
    "             order = df_objects_other[col].value_counts().index, ax=ax[1])\n",
    "    ax[1].set_title(f\"\"\"{col} manipulated\"\"\")\n",
    "    \n",
    "    # graph 3\n",
    "    sns.countplot(data = df_translated, x = col, color = 'green',\n",
    "             order = df_translated[col].value_counts().index, ax=ax[2])\n",
    "    ax[2].set_title(f\"\"\"{col} translated\"\"\")\n",
    "    \n",
    "    for ax in fig.axes:\n",
    "        plt.sca(ax)\n",
    "        plt.xticks(rotation=90)\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa9660",
   "metadata": {},
   "source": [
    "<h3> Missing values </h3> \n",
    "\n",
    "<h4> Detecting missing values </h4> \n",
    "Next we analysed the missing values, and found out that only three columns contained the 3394 missing values using the following code:\n",
    "\n",
    "~~~\n",
    "print(df.isnull().sum().sum())\n",
    "~~~\n",
    "\n",
    "So we looked deeper into the columns <em>job</em>, <em> type_of_employment </em> and <em> nationality </em> leading us to the following numbers: \n",
    "\n",
    "~~~\n",
    "                    count_nas  percent_nas\n",
    "job                      1449         5.12\n",
    "type_of_employment       1442         5.10\n",
    "nationality               503         1.78\n",
    "~~~\n",
    "\n",
    "This reveals that 5% of the values are contained in the columns <em>job</em>, and <em>type_of_employment</em>. We  investigated these missing values by creating flagging columns, see Code 2.2.1.1, in order to identify any patterns. The analysis revealed that 26,348 rows includes missing values, and 18 of those rows had 3 missing values per row.\n",
    "\n",
    "~~~\n",
    "Count of missing values per row\n",
    "0    26348\n",
    "1      492\n",
    "2     1424\n",
    "3       18\n",
    "~~~\n",
    "\n",
    "To avoid loosing the existing data included in the rows with missing values we decided to impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 2.2.1.1\n",
    "\n",
    "#creating empty dataframe\n",
    "values_df= pd.DataFrame()   \n",
    "\n",
    "# adding count of missing values and missing percentage for every column   \n",
    "values_df['count_nas'] = df_translated.isnull().sum(axis=0)\n",
    "values_df['percent_nas'] = (df_translated.isnull().mean(axis=0)*100).round(decimals=2)\n",
    "\n",
    "# printing how many missing values for each column\n",
    "print(f\"\"\"\\nmissing values for each column:\n",
    "{values_df.loc[:,:][values_df.loc[:, 'count_nas']>0]}\\n\"\"\")\n",
    "\n",
    "# creating flagging columns\n",
    "DATAFRAME = df_translated\n",
    "\n",
    "# developing a loop to automatically flag missing values\n",
    "for col in DATAFRAME:\n",
    "\n",
    "    if DATAFRAME[col].isnull().astype(int).sum() > 0:\n",
    "        DATAFRAME['m_'+col] = DATAFRAME[col].isnull().astype(int)\n",
    "\n",
    "df_translated['m_sum'] = df_translated['m_job'] + df_translated['m_type_of_employment'] + df_translated['m_nationality']\n",
    "\n",
    "#printing summary of missing values\n",
    "print(df_translated['m_sum'].value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85861b",
   "metadata": {},
   "source": [
    "<h4> Imputing values </h4> \n",
    "\n",
    "Our imputation strategy was based on the type of the data; since all missing values were categorical, we decided to input the mode, however, for job and nationality we went with a slightly different strategy. \n",
    "\n",
    "We discovered that the type of job was highly dependent on the level of education, thus imputing the mode of job would be inefficient so, we decided to impute the mode based on level of education.\n",
    "\n",
    "Additionally, we decided impute missing values in nationality based on race.\n",
    "\n",
    "Finally, had we encountered any missing numerical values, we decided to impute the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca55ca0",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "# Code 2.2.2.1\n",
    "df_translated[['level_education', 'job']].value_counts().sort_index()['Associate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea62530",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "# Code 2.2.2.2\n",
    "df_translated[['level_education', 'job']].value_counts().sort_index()['University']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b1503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 2.2.2.3\n",
    "\n",
    "# numerical columns --> impute with the mean\n",
    "for col in df_translated[num_cols].columns:\n",
    "    numerical_mean = int(df_translated[col].mean())\n",
    "    df_translated[col].fillna(value = numerical_mean,\n",
    "                             inplace = True)\n",
    "    \n",
    "#categorical columns\n",
    "# these columns we want to change by hand\n",
    "handmade_columns = ['job', 'nationality']\n",
    "\n",
    "# changing not handmade columns --> mode\n",
    "for col in df_translated[cat_cols].drop(labels=handmade_columns, axis=1).columns:\n",
    "    # getting the mode\n",
    "    categorical_mode = df_translated[col].mode()[0]\n",
    "    # filling NAs with Mode\n",
    "    df_translated[col].fillna(value = categorical_mode,\n",
    "                             inplace = True) \n",
    "\n",
    "# handmade changes\n",
    "# imputing nationlity using race\n",
    "dict_nat_race = {}\n",
    "\n",
    "# for all races get most appearing nationality\n",
    "for race in df_translated['race'].unique():\n",
    "    input_value = df_translated[['race', 'nationality']].value_counts()[race].idxmax()\n",
    "    dict_nat_race[race] = input_value\n",
    "       \n",
    "for race in dict_nat_race:\n",
    "    df_translated.loc[(df_translated['race'] == race) & (df_translated['nationality'].isnull()),'nationality'] = dict_nat_race[race]\n",
    "\n",
    "# imputing job using education\n",
    "dict_job_ed = {}\n",
    "\n",
    "for ed in df_translated['level_education'].unique():\n",
    "    input_value = df_translated[['level_education', 'job']].value_counts()[ed].idxmax()\n",
    "    dict_job_ed[ed] = input_value\n",
    "        \n",
    "for ed in dict_job_ed:\n",
    "    df_translated.loc[(df_translated['level_education'] == ed) & (df_translated['job'].isnull()),'job'] = dict_job_ed[ed]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba87d30",
   "metadata": {},
   "source": [
    " <h2> Creating Dummy-variables for categorical columns</h2>\n",
    " \n",
    " To compare the impact of the summarization from values to <em> other </em> we created dummy variables for both, the new and the original dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 2.3.1\n",
    "\n",
    "# creating dummy variables for categorical columns  \n",
    "for col in df_translated.select_dtypes(include=['object']).columns:\n",
    "    df_translated = pd.concat(objs=[df_translated, pd.get_dummies(data       = df_translated[col], \n",
    "                                                                  drop_first = True, \n",
    "                                                                  prefix     = f\"\"\"d_{col}\"\"\")\n",
    "                               ], \n",
    "                          axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31274f1c",
   "metadata": {},
   "source": [
    " <h2> Creating Bins for numerical columns </h2>\n",
    " \n",
    "Since the Naives Bayes Classification only works with categorical values, we translated our numerical columns into bins. This was done by splitting the columns into 4 categories of the same size. \n",
    "However, this method did not work in all cases, for example return of investment as it has a lot of zeros and so it was not possible to split into quantiles, hence we decided to split by unique values, see Code 2.4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ce7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 2.4.1\n",
    "\n",
    "# example for return of investment --> jumps into except case\n",
    "try:\n",
    "    df_translated['return_of_investment'] = pd.qcut(x=df_translated['return_of_investment'], q=4, labels = [0, 1, 2, 3])\n",
    "except:\n",
    "    # get the quantile values for the column\n",
    "    quantiles = np.quantile(df_translated['return_of_investment'].unique(), q=[0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "    # produce the bins\n",
    "    df_translated['RoI_Bins'] = pd.cut(x = df_translated['return_of_investment'], bins=quantiles, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92bfaa9",
   "metadata": {},
   "source": [
    " <h2> Create train and test sets </h2>\n",
    " \n",
    " The first step in creating the sets was to isolate the target value, <em> yearly_income_bigger_50k </em>. We did that by slicing our dataframe, and then we dropped that column from the original dataframe.\n",
    " To create the sets, we used the train-test-split module from the sklearn library, stratifying the data with the target variable ensuring that both test and train sets included income higher and lower than $50k.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 2.5.1\n",
    "\n",
    "# isolating target value into new df\n",
    "# getting columns for y\n",
    "if 'd_yearly_income_bigger_50k_>50K' in df_translated.columns:\n",
    "    y_cols = ['d_yearly_income_bigger_50k_>50K', 'yearly_income_bigger_50k']\n",
    "elif 'd_yearly_income_bigger_50k_<=50K':\n",
    "    y_cols = ['d_yearly_income_bigger_50k_<=50K', 'yearly_income_bigger_50k']\n",
    "else:\n",
    "    print('Missing column for prediction')\n",
    "    \n",
    "y = df_translated.loc[:,y_cols]\n",
    "# dropping target value from original df\n",
    "df = df_translated.drop(labels=y_cols, axis=1)\n",
    "\n",
    "\n",
    "#creating test and train sets using sklearn\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_translated, y, test_size=test_size, stratify=y, random_state=1)\n",
    "\n",
    "\n",
    "X_train.to_excel(excel_writer='train_x.xlsx', sheet_name='train_x')\n",
    "X_test.to_excel(excel_writer='test_x.xlsx', sheet_name='test_x')\n",
    "y_train.to_excel(excel_writer='train_y.xlsx', sheet_name='train_y')\n",
    "y_test.to_excel(excel_writer='test_y.xlsx', sheet_name='test_y')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
